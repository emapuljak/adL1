{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "difficult-florist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Model,model_from_json\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, BatchNormalization, Activation, Concatenate, Dropout, Layer\n",
    "from tensorflow.keras.layers import ReLU, LeakyReLU\n",
    "from tensorflow.keras import backend as K\n",
    "from qkeras import QDense, QActivation\n",
    "import math\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorboard import program\n",
    "import os\n",
    "import pathlib\n",
    "#import tensorflow_model_optimization as tfmot\n",
    "#tsk = tfmot.sparsity.keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "from functions import preprocess_anomaly_data, custom_loss_negative, custom_loss_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "equipped-manual",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "exceptional-ridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('Delphes_dataset_HALF.h5', 'r')\n",
    "X_train_flatten = np.array(file['X_train_flatten'])\n",
    "X_test_flatten = np.array(file['X_test_flatten'])\n",
    "X_val_flatten = np.array(file['X_val_flatten'])\n",
    "\n",
    "X_train_scaled = np.array(file['X_train_scaled'])\n",
    "#X_test_scaled = np.array(file['X_test_scaled'])\n",
    "X_val_scaled = np.array(file['X_val_scaled'])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-topic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 56)]              0         \n",
      "_________________________________________________________________\n",
      "block_1_act (Activation)     (None, 56)                0         \n",
      "_________________________________________________________________\n",
      "bn_1 (BatchNormalization)    (None, 56)                224       \n",
      "_________________________________________________________________\n",
      "block_2_dense (Dense)        (None, 32)                1792      \n",
      "_________________________________________________________________\n",
      "bn_2 (BatchNormalization)    (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "block_2_act (Activation)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "block_3_dense (Dense)        (None, 16)                512       \n",
      "_________________________________________________________________\n",
      "bn_3 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "block_3_act (Activation)     (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "output_encoder (Dense)       (None, 3)                 51        \n",
      "_________________________________________________________________\n",
      "block_4_dense (Dense)        (None, 16)                48        \n",
      "_________________________________________________________________\n",
      "bn_4 (BatchNormalization)    (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "block_4_act (Activation)     (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "block_5_dense (Dense)        (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "bn_5 (BatchNormalization)    (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "block_5_act (Activation)     (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "output_dense (Dense)         (None, 56)                1848      \n",
      "_________________________________________________________________\n",
      "output_act (Activation)      (None, 56)                0         \n",
      "=================================================================\n",
      "Total params: 5,371\n",
      "Trainable params: 5,067\n",
      "Non-trainable params: 304\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 3\n",
    "input_shape = 56\n",
    "\n",
    "#encoder\n",
    "inputArray = Input(shape=(input_shape))\n",
    "x = Activation('linear', name='block_1_act')(inputArray)\n",
    " #   else QActivation(f'quantized_bits(16,6,1)')(inputArray)\n",
    "x = BatchNormalization(name='bn_1')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_2_dense')(x)\n",
    "x = BatchNormalization(name='bn_2')(x)\n",
    "x = Activation('relu', name='block_2_act')(x)\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_3_dense')(x)\n",
    "x = BatchNormalization(name='bn_3')(x)\n",
    "x = Activation('relu', name='block_3_act')(x)\n",
    "encoder = Dense(latent_dim, kernel_initializer=tf.keras.initializers.HeUniform(),name='output_encoder')(x)\n",
    "#x = BatchNormalization()(x)\n",
    "\n",
    "#decoder\n",
    "x = Dense(16, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_4_dense')(encoder)\n",
    "x = BatchNormalization(name='bn_4')(x)\n",
    "x = Activation('relu', name='block_4_act')(x)\n",
    "x = Dense(32, kernel_initializer=tf.keras.initializers.HeUniform(),use_bias=False, name='block_5_dense')(x)\n",
    "x = BatchNormalization(name='bn_5')(x)\n",
    "x = Activation('relu', name='block_5_act')(x)\n",
    "x = Dense(input_shape, kernel_initializer=tf.keras.initializers.HeUniform(), name='output_dense')(x)\n",
    "decoder = Activation('linear', name='output_act')(x)\n",
    "\n",
    "#create autoencoder\n",
    "autoencoder = Model(inputs = inputArray, outputs=decoder)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-omega",
   "metadata": {},
   "source": [
    "### Load signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "apart-defeat",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/epuljak/conda/envs/tf-gpu/lib/python3.8/site-packages/sklearn/base.py:310: UserWarning: Trying to unpickle estimator StandardScaler from version 0.22.1 when using version 0.24.0. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "ato4l = h5py.File('Ato4l_lepFilter_13TeV.h5', 'r')\n",
    "ato4l = ato4l['Particles'][:]\n",
    "ato4l = ato4l[:,:,:-1]\n",
    "\n",
    "import joblib\n",
    "pT_scaler = joblib.load('pt_scaled_VAE_new.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "opponent-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled_ato4l, test_notscaled_ato4l = preprocess_anomaly_data(pT_scaler, ato4l)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaptive-fashion",
   "metadata": {},
   "source": [
    "### Set objective and  compile the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "twenty-zealand",
   "metadata": {},
   "outputs": [],
   "source": [
    "bsm_data = test_notscaled_ato4l #input - data without any preprocessing\n",
    "#obj = roc_objective(autoencoder, X_test_flatten[:1000], bsm_data)\n",
    "autoencoder.compile(optimizer=keras.optimizers.Adam(), loss=custom_loss_training, run_eagerly=True) # just to make sure it runs in eager"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "needed-cover",
   "metadata": {},
   "source": [
    "### Override AutoQKeras classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mexican-error",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom_AutoQKeras import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "active-commerce",
   "metadata": {},
   "source": [
    "### Set AutoQKeras parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "shared-helping",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qkeras import *\n",
    "from qkeras.utils import model_quantize\n",
    "from qkeras.qtools import run_qtools\n",
    "from qkeras.qtools import settings as qtools_settings\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dedicated-iraqi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:XLA_GPU:1', device_type='XLA_GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices()\n",
    "for d in physical_devices:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "close-landing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/epuljak/conda/envs/tf-gpu/lib/python3.8/site-packages/qkeras/qtools/qgraph.py:189: Tensor.experimental_ref (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use ref() instead.\n",
      "{'block_1_act': {'energy': {'inputs': 53.25,\n",
      "                            'op_cost': 0.0,\n",
      "                            'outputs': 106.51,\n",
      "                            'parameters': 0.0},\n",
      "                 'total': 106.51},\n",
      " 'block_2_act': {'energy': {'inputs': 60.86,\n",
      "                            'op_cost': 0.0,\n",
      "                            'outputs': 60.86,\n",
      "                            'parameters': 0.0},\n",
      "                 'total': 60.86},\n",
      " 'block_2_dense': {'energy': {'inputs': 106.51,\n",
      "                              'op_cost': 8243.2,\n",
      "                              'outputs': 60.86,\n",
      "                              'parameters': 3408.2},\n",
      "                   'total': 11757.91},\n",
      " 'block_3_act': {'energy': {'inputs': 30.43,\n",
      "                            'op_cost': 0.0,\n",
      "                            'outputs': 30.43,\n",
      "                            'parameters': 0.0},\n",
      "                 'total': 30.43},\n",
      " 'block_3_dense': {'energy': {'inputs': 60.86,\n",
      "                              'op_cost': 2355.2,\n",
      "                              'outputs': 30.43,\n",
      "                              'parameters': 973.77},\n",
      "                   'total': 3389.83},\n",
      " 'block_4_act': {'energy': {'inputs': 30.43,\n",
      "                            'op_cost': 0.0,\n",
      "                            'outputs': 30.43,\n",
      "                            'parameters': 0.0},\n",
      "                 'total': 30.43},\n",
      " 'block_4_dense': {'energy': {'inputs': 7.61,\n",
      "                              'op_cost': 220.8,\n",
      "                              'outputs': 30.43,\n",
      "                              'parameters': 91.29},\n",
      "                   'total': 319.70000000000005},\n",
      " 'block_5_act': {'energy': {'inputs': 60.86,\n",
      "                            'op_cost': 0.0,\n",
      "                            'outputs': 60.86,\n",
      "                            'parameters': 0.0},\n",
      "                 'total': 60.86},\n",
      " 'block_5_dense': {'energy': {'inputs': 30.43,\n",
      "                              'op_cost': 2355.2,\n",
      "                              'outputs': 60.86,\n",
      "                              'parameters': 973.77},\n",
      "                   'total': 3359.3999999999996},\n",
      " 'bn_1': {'energy': {'inputs': 106.51,\n",
      "                     'op_cost': 414.4,\n",
      "                     'outputs': 106.51,\n",
      "                     'parameters': 426.02},\n",
      "          'total': 426.02},\n",
      " 'bn_2': {'energy': {'inputs': 60.86,\n",
      "                     'op_cost': 236.8,\n",
      "                     'outputs': 60.86,\n",
      "                     'parameters': 243.44},\n",
      "          'total': 243.44},\n",
      " 'bn_3': {'energy': {'inputs': 30.43,\n",
      "                     'op_cost': 118.4,\n",
      "                     'outputs': 30.43,\n",
      "                     'parameters': 121.72},\n",
      "          'total': 121.72},\n",
      " 'bn_4': {'energy': {'inputs': 30.43,\n",
      "                     'op_cost': 118.4,\n",
      "                     'outputs': 30.43,\n",
      "                     'parameters': 121.72},\n",
      "          'total': 121.72},\n",
      " 'bn_5': {'energy': {'inputs': 60.86,\n",
      "                     'op_cost': 236.8,\n",
      "                     'outputs': 60.86,\n",
      "                     'parameters': 243.44},\n",
      "          'total': 243.44},\n",
      " 'output_act': {'energy': {'inputs': 106.51,\n",
      "                           'op_cost': 0.0,\n",
      "                           'outputs': 106.51,\n",
      "                           'parameters': 0.0},\n",
      "                'total': 106.51},\n",
      " 'output_dense': {'energy': {'inputs': 60.86,\n",
      "                             'op_cost': 8243.2,\n",
      "                             'outputs': 106.51,\n",
      "                             'parameters': 3514.7},\n",
      "                  'total': 11818.76},\n",
      " 'output_encoder': {'energy': {'inputs': 30.43,\n",
      "                               'op_cost': 220.8,\n",
      "                               'outputs': 7.61,\n",
      "                               'parameters': 98.9},\n",
      "                    'total': 350.13}}\n",
      "\n",
      "Total energy: 0.03 uJ\n"
     ]
    }
   ],
   "source": [
    "reference_internal = \"fp32\"\n",
    "reference_accumulator = \"fp32\"\n",
    "\n",
    "q = run_qtools.QTools(\n",
    "  autoencoder,\n",
    "  # energy calculation using a given process\n",
    "  # \"horowitz\" refers to 45nm process published at\n",
    "  # M. Horowitz, \"1.1 Computing's energy problem (and what we can do about\n",
    "  # it), \"2014 IEEE International Solid-State Circuits Conference Digest of\n",
    "  # Technical Papers (ISSCC), San Francisco, CA, 2014, pp. 10-14, \n",
    "  # doi: 10.1109/ISSCC.2014.6757323.\n",
    "  process=\"horowitz\",\n",
    "  # quantizers for model input\n",
    "  source_quantizers=[quantized_bits(16, 6, 1)],\n",
    "  is_inference=False,\n",
    "  # absolute path (including filename) of the model weights\n",
    "  # in the future, we will attempt to optimize the power model\n",
    "  # by using weight information, although it can be used to further\n",
    "  # optimize QBatchNormalization.\n",
    "  weights_path=None,\n",
    "  # keras_quantizer to quantize weight/bias in un-quantized keras layers\n",
    "  keras_quantizer=reference_internal,\n",
    "  # keras_quantizer to quantize MAC in un-quantized keras layers\n",
    "  keras_accumulator=reference_accumulator,\n",
    "  # whether calculate baseline energy\n",
    "  for_reference=True)\n",
    "  \n",
    "# caculate energy of the derived data type map.\n",
    "energy_dict = q.pe(\n",
    "    # whether to store parameters in dram, sram, or fixed\n",
    "    weights_on_memory=\"sram\",\n",
    "    # store activations in dram or sram\n",
    "    activations_on_memory=\"sram\",\n",
    "    # minimum sram size in number of bits. Let's assume a 16MB SRAM.\n",
    "    min_sram_size=8*16*1024*1024,\n",
    "    rd_wr_on_io=False)\n",
    "\n",
    "# get stats of energy distribution in each layer\n",
    "energy_profile = q.extract_energy_profile(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "# extract sum of energy of each layer according to the rule specified in\n",
    "# qtools_settings.cfg.include_energy\n",
    "total_energy = q.extract_energy_sum(\n",
    "    qtools_settings.cfg.include_energy, energy_dict)\n",
    "\n",
    "pprint.pprint(energy_profile)\n",
    "print()\n",
    "print(\"Total energy: {:.2f} uJ\".format(total_energy / 1000000.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facial-american",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantization_config = {\n",
    "        \"kernel\": {\n",
    "                \"quantized_bits(2,1,1,alpha=1.0)\": 2,\n",
    "                \"quantized_bits(4,2,1,alpha=1.0)\": 4,\n",
    "                \"quantized_bits(6,2,1,alpha=1.0)\": 6,\n",
    "                \"quantized_bits(8,3,1,alpha=1.0)\": 8,\n",
    "                \"quantized_bits(10,3,1,alpha=1.0)\": 10,\n",
    "                \"quantized_bits(12,4,1,alpha=1.0)\": 12,\n",
    "                \"quantized_bits(14,4,1,alpha=1.0)\": 14,\n",
    "                \"quantized_bits(16,6,1,alpha=1.0)\": 16\n",
    "        },\n",
    "        \"bias\": {\n",
    "                \"quantized_bits(2,1,1)\": 2,\n",
    "                \"quantized_bits(4,2,1)\": 4,\n",
    "                \"quantized_bits(6,2,1)\": 6,\n",
    "                \"quantized_bits(8,3,1)\": 8\n",
    "        },\n",
    "        \"activation\": {\n",
    "                \"quantized_relu(2,1)\": 2,\n",
    "                \"quantized_relu(3,1)\": 3,\n",
    "                \"quantized_relu(4,2)\": 4,\n",
    "                \"quantized_relu(6,2)\": 6,\n",
    "                \"quantized_relu(8,3)\": 8,\n",
    "                \"quantized_relu(10,3)\": 10,\n",
    "                \"quantized_relu(12,4)\": 12,\n",
    "                \"quantized_relu(14,4)\": 14,\n",
    "                \"quantized_relu(16,6)\": 16\n",
    "        },\n",
    "        \"linear\": {\n",
    "                \"quantized_bits(16,6)\": 16\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "senior-airplane",
   "metadata": {},
   "outputs": [],
   "source": [
    "limit = {\n",
    "    \"Dense\": [16, 8, 16],\n",
    "    \"Activation\": [16]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "saving-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal = {\n",
    "    \"type\": \"energy\",\n",
    "    \"params\": {\n",
    "        \"delta_p\": 8.0,\n",
    "        \"delta_n\": 8.0,\n",
    "        \"rate\": 4.0, # a try\n",
    "        \"stress\": 0.6, # a try\n",
    "        \"process\": \"horowitz\",\n",
    "        \"parameters_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"activations_on_memory\": [\"sram\", \"sram\"],\n",
    "        \"rd_wr_on_io\": [False, False],\n",
    "        \"min_sram_size\": [0, 0],\n",
    "        \"source_quantizers\": [\"fp16\"],\n",
    "        \"reference_internal\": \"fp16\",\n",
    "        \"reference_accumulator\": \"fp16\"\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "collected-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "odir='autoqkeras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "running-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = {\n",
    "    \"output_dir\": \"{}/\".format(odir),\n",
    "    \"goal\": goal,\n",
    "    \"quantization_config\": quantization_config,\n",
    "    \"learning_rate_optimizer\": False,\n",
    "    \"transfer_weights\": False,\n",
    "    \"mode\": \"bayesian\",\n",
    "    #\"score_metric\": \"val_roc_objective_val\",\n",
    "    \"seed\": 42,\n",
    "    \"limit\": limit,\n",
    "    \"tune_filters\": \"layer\",\n",
    "    \"tune_filters_exceptions\": \"^output.*\",\n",
    "    \"layer_indexes\": [1,3,5,6,8,9,10,12,13,15,16,17],\n",
    "    \"max_trials\": 130,\n",
    "    \"blocks\": [\n",
    "          \"block_1_.*$\",\n",
    "          \"block_2_.*$\",\n",
    "          \"block_3_.*$\",\n",
    "          \"output_encoder$\",\n",
    "          \"block_4_.*$\",\n",
    "          \"block_5_.*$\",\n",
    "          \"output_dense$\",\n",
    "          \"output_act$\",],\n",
    "    \"schedule_block\": \"cost\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "given-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quantizing layers: ['block_1_act', 'block_2_dense', 'block_2_act', 'block_3_dense', 'block_3_act', 'output_encoder', 'block_4_dense', 'block_4_act', 'block_5_dense', 'block_5_act', 'output_dense', 'output_act']\n"
     ]
    }
   ],
   "source": [
    "print(\"quantizing layers:\", [autoencoder.layers[i].name for i in run_config[\"layer_indexes\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "charming-power",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "\n",
    "callbacks=[]\n",
    "#if pruning=='pruned':\n",
    " #   callbacks.append(tfmot.sparsity.keras.UpdatePruningStep())\n",
    "callbacks.append(ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, verbose=1, mode='auto', min_delta=0.0001, cooldown=2, min_lr=1E-6))\n",
    "#callbacks.append(TerminateOnNaN())\n",
    "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_best_only=True))\n",
    "#callbacks.append(tf.keras.callbacks.ModelCheckpoint(filepath='{}/AUTOQKERAS_best_weights.h5'.format(odir),monitor=\"val_loss\",verbose=0,save_weights_only=True))\n",
    "callbacks.append(tf.keras.callbacks.EarlyStopping(monitor='val_loss',verbose=1, patience=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "handled-laptop",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 25\n",
    "BATCH_SIZE = 1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olympic-carry",
   "metadata": {},
   "source": [
    "### Run search with AutoQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ongoing-clause",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "premium-subsection",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoqk = Custom_AutoQKerasScheduler(autoencoder,metrics=[custom_loss_negative], X_test = X_test_flatten[:3000000], bsm_data = bsm_data,\\\n",
    "                             custom_objects={}, debug=False, **run_config)\n",
    "autoqk.fit(X_train_flatten[:3000000], X_train_scaled[:3000000],\\\n",
    "           validation_data=(X_val_flatten[:3000000], X_val_scaled[:3000000]),\\\n",
    "           batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "substantial-spirit",
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-cooperation",
   "metadata": {},
   "outputs": [],
   "source": [
    "qmodel = autoqk.get_best_model()\n",
    "qmodel.summary()\n",
    "save_model('best_pretrain_objective_roc', qmodel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
